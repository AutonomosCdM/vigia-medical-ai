#!/usr/bin/env python3
"""
Setup MedGemma con Ollama - Alternativa simple sin autenticaciÃ³n
Instala y configura MedGemma usando Ollama para evitar complejidades de Hugging Face.

Uso:
    python scripts/setup_medgemma_ollama.py --model 27b --install
    python scripts/setup_medgemma_ollama.py --model 4b --test
"""

import os
import sys
import argparse
import asyncio
import subprocess
import json
from pathlib import Path
from typing import Dict, Any, Optional

# Agregar path del proyecto
sys.path.append(str(Path(__file__).parent.parent))

from vigia_detect.utils.secure_logger import SecureLogger

logger = SecureLogger("medgemma_ollama_setup")


class MedGemmaOllamaSetup:
    """Configurador de MedGemma usando Ollama."""
    
    def __init__(self):
        self.models_map = {
            "4b": "alibayram/medgemma",      # 4B multimodal
            "27b": "symptoma/medgemma3"      # 27B text-only optimizado
        }
    
    def check_ollama_installed(self) -> bool:
        """Verificar si Ollama estÃ¡ instalado."""
        try:
            result = subprocess.run(["ollama", "--version"], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print(f"âœ… Ollama instalado: {result.stdout.strip()}")
                return True
            else:
                print("âŒ Ollama no responde correctamente")
                return False
        except FileNotFoundError:
            print("âŒ Ollama no estÃ¡ instalado")
            return False
    
    def install_ollama(self):
        """Instalar Ollama."""
        print("ğŸ“¦ Instalando Ollama...")
        
        # Detectar sistema operativo
        import platform
        system = platform.system().lower()
        
        if system == "darwin":  # macOS
            print("   Instalando en macOS...")
            os.system("curl -fsSL https://ollama.ai/install.sh | sh")
        elif system == "linux":
            print("   Instalando en Linux...")
            os.system("curl -fsSL https://ollama.ai/install.sh | sh")
        else:
            print("âŒ Sistema no soportado para instalaciÃ³n automÃ¡tica")
            print("   Instala manualmente desde https://ollama.ai")
            return False
        
        return self.check_ollama_installed()
    
    def list_available_models(self):
        """Listar modelos MedGemma disponibles."""
        print("ğŸ“‹ Modelos MedGemma disponibles via Ollama:")
        print()
        
        for size, model_name in self.models_map.items():
            print(f"ğŸ”¸ {size}: {model_name}")
            if size == "4b":
                print("   Tipo: Multimodal (texto + imagen)")
                print("   ParÃ¡metros: 4B")
                print("   Memoria: ~8GB RAM")
                print("   Ventajas: MÃ¡s rÃ¡pido, soporte de imÃ¡genes")
            else:
                print("   Tipo: Solo texto")
                print("   ParÃ¡metros: 27B") 
                print("   Memoria: ~16GB RAM")
                print("   Ventajas: MÃ¡s preciso para anÃ¡lisis mÃ©dico")
            
            # Verificar si estÃ¡ instalado
            if self.check_model_installed(size):
                print("   Estado: âœ… Instalado")
            else:
                print("   Estado: âŒ No instalado")
            print()
    
    def check_model_installed(self, model_size: str) -> bool:
        """Verificar si un modelo estÃ¡ instalado."""
        if model_size not in self.models_map:
            return False
            
        model_name = self.models_map[model_size]
        
        try:
            result = subprocess.run(["ollama", "list"], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                return model_name in result.stdout
            return False
        except Exception:
            return False
    
    def install_model(self, model_size: str) -> bool:
        """Instalar modelo MedGemma."""
        if model_size not in self.models_map:
            print(f"âŒ Modelo '{model_size}' no vÃ¡lido. Opciones: {list(self.models_map.keys())}")
            return False
        
        if not self.check_ollama_installed():
            print("âŒ Ollama no estÃ¡ instalado. Ejecuta: --install-ollama")
            return False
        
        model_name = self.models_map[model_size]
        print(f"ğŸ“¥ Instalando modelo {model_name}...")
        print(f"   â³ Esto puede tomar varios minutos dependiendo de tu conexiÃ³n...")
        
        try:
            # Ejecutar ollama pull
            result = subprocess.run(["ollama", "pull", model_name], 
                                  capture_output=False, text=True)
            
            if result.returncode == 0:
                print(f"âœ… Modelo {model_name} instalado exitosamente")
                return True
            else:
                print(f"âŒ Error instalando modelo: {result}")
                return False
                
        except Exception as e:
            print(f"âŒ Error ejecutando ollama pull: {e}")
            return False
    
    def test_model(self, model_size: str) -> bool:
        """Probar modelo instalado."""
        if not self.check_model_installed(model_size):
            print(f"âŒ Modelo {model_size} no estÃ¡ instalado")
            return False
        
        model_name = self.models_map[model_size]
        print(f"ğŸ§ª Probando modelo {model_name}...")
        
        # Prompt de prueba mÃ©dico
        test_prompt = "Â¿CuÃ¡les son los signos de una lesiÃ³n por presiÃ³n grado 2?"
        
        try:
            # Crear comando ollama run
            cmd = ["ollama", "run", model_name, test_prompt]
            
            print(f"   Prompt: {test_prompt}")
            print("   ğŸ¤– Respuesta:")
            print("   " + "-" * 50)
            
            # Ejecutar y mostrar respuesta en tiempo real
            result = subprocess.run(cmd, text=True)
            
            if result.returncode == 0:
                print("   " + "-" * 50)
                print("âœ… Modelo funcionando correctamente")
                return True
            else:
                print(f"âŒ Error ejecutando modelo: {result}")
                return False
                
        except Exception as e:
            print(f"âŒ Error probando modelo: {e}")
            return False
    
    def get_model_info(self, model_size: str) -> Dict[str, Any]:
        """Obtener informaciÃ³n del modelo."""
        if not self.check_model_installed(model_size):
            return {"installed": False}
        
        model_name = self.models_map[model_size]
        
        try:
            # Obtener informaciÃ³n con ollama show
            result = subprocess.run(["ollama", "show", model_name], 
                                  capture_output=True, text=True)
            
            if result.returncode == 0:
                return {
                    "installed": True,
                    "model_name": model_name,
                    "info": result.stdout
                }
            else:
                return {"installed": False, "error": result.stderr}
                
        except Exception as e:
            return {"installed": False, "error": str(e)}
    
    def get_recommendation(self) -> str:
        """Obtener recomendaciÃ³n de modelo segÃºn sistema."""
        import psutil
        
        # Obtener memoria RAM disponible
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        print(f"ğŸ’¾ Memoria RAM disponible: {memory_gb:.1f} GB")
        
        if memory_gb >= 32:
            print("ğŸ’¡ RecomendaciÃ³n: Modelo 27b (mejor precisiÃ³n mÃ©dica)")
            return "27b"
        elif memory_gb >= 16:
            print("ğŸ’¡ RecomendaciÃ³n: Modelo 27b (con swap si es necesario)")
            return "27b"
        else:
            print("ğŸ’¡ RecomendaciÃ³n: Modelo 4b (mÃ¡s eficiente)")
            return "4b"


async def main():
    """FunciÃ³n principal."""
    parser = argparse.ArgumentParser(description="Configurar MedGemma con Ollama")
    parser.add_argument("--model", choices=["4b", "27b"], 
                       help="Modelo a instalar/verificar")
    parser.add_argument("--install", action="store_true",
                       help="Instalar modelo")
    parser.add_argument("--test", action="store_true",
                       help="Probar modelo")
    parser.add_argument("--list", action="store_true",
                       help="Listar modelos disponibles")
    parser.add_argument("--install-ollama", action="store_true",
                       help="Instalar Ollama")
    parser.add_argument("--check", action="store_true",
                       help="Verificar instalaciÃ³n")
    
    args = parser.parse_args()
    
    setup = MedGemmaOllamaSetup()
    
    print("ğŸ¤– MedGemma Ollama Setup")
    print("=" * 50)
    
    if args.install_ollama:
        setup.install_ollama()
        return
    
    if args.list:
        setup.list_available_models()
        return
    
    if args.check:
        if setup.check_ollama_installed():
            print("âœ… Ollama estÃ¡ funcionando correctamente")
        else:
            print("âŒ Ollama no estÃ¡ disponible. Ejecuta: --install-ollama")
        return
    
    # Verificar que Ollama estÃ© instalado
    if not setup.check_ollama_installed():
        print("âŒ Ollama no estÃ¡ instalado. Ejecuta: --install-ollama")
        return
    
    if not args.model:
        # Mostrar recomendaciÃ³n
        recommended = setup.get_recommendation()
        print(f"ğŸ’¡ Usar: --model {recommended} --install")
        return
    
    if args.install:
        print(f"ğŸ“¥ Instalando modelo {args.model}...")
        success = setup.install_model(args.model)
        if not success:
            return
    
    if args.test:
        print(f"ğŸ§ª Probando modelo {args.model}...")
        success = setup.test_model(args.model)
        if success:
            print("âœ… Modelo listo para usar con Ollama")
        else:
            print("âŒ Modelo no funciona correctamente")


if __name__ == "__main__":
    asyncio.run(main())